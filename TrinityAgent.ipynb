{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd28b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 1. TRAINING SPECIALIST AGENTS ===\n",
      ">> Training Trend Agent (Striker)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/04/4l6xjjw976j6gj89kg1qrrf80000gn/T/ipykernel_3747/3422824058.py:54: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(self.tickers, start=start, end=end, progress=False)\n",
      "/var/folders/04/4l6xjjw976j6gj89kg1qrrf80000gn/T/ipykernel_3747/3422824058.py:61: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  ohlc = yf.download(CONFIG['TARGET_ASSET'], start=start, end=end, progress=False)\n",
      "/opt/anaconda3/envs/ml/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Training Mean Rev Agent (Midfielder)...\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# 1. GLOBAL CONFIGURATION\n",
    "# ==========================================\n",
    "CONFIG = {\n",
    "    # Assets\n",
    "    \"TICKERS\": [\"SPY\", \"IWM\", \"^VIX\", \"SHY\"], \n",
    "    \"TARGET_ASSET\": \"SPY\",\n",
    "    \n",
    "    # Dates\n",
    "    \"TRAIN_START\": \"2015-01-01\",\n",
    "    \"TRAIN_END\": \"2023-12-31\",\n",
    "    \"TEST_START\": \"2024-01-02\",\n",
    "    \"TEST_END\": \"2025-01-01\",\n",
    "    \n",
    "    # Crisis Training Periods (The \"Frankenstein\" Dataset)\n",
    "    \"CRISIS_PERIODS\": [\n",
    "        (\"2007-08-01\", \"2009-06-01\"), # GFC\n",
    "        (\"2018-10-01\", \"2019-01-01\"), # Volmageddon\n",
    "        (\"2020-01-01\", \"2020-05-01\"), # COVID\n",
    "        (\"2022-01-01\", \"2022-12-31\"), # Bear\n",
    "    ],\n",
    "    \n",
    "    # Feature Engineering\n",
    "    \"WINDOW_SIZE\": 20,\n",
    "    \"BB_STD\": 2.0,\n",
    "    \n",
    "    # Agent Parameters (Optimized for Fast Demo Training)\n",
    "    \"LEARNING_RATE\": 3e-4,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"TRAINING_STEPS\": 30000, # Short training for demonstration\n",
    "    \"INITIAL_BALANCE\": 100000,\n",
    "    \"FEES\": 0.0005\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA PROCESSOR (MULTI-REGIME)\n",
    "# ==========================================\n",
    "class DataProcessor:\n",
    "    def __init__(self, tickers):\n",
    "        self.tickers = tickers\n",
    "        \n",
    "    def download(self, start, end):\n",
    "        try:\n",
    "            data = yf.download(self.tickers, start=start, end=end, progress=False)\n",
    "            if isinstance(data.columns, pd.MultiIndex):\n",
    "                if 'Close' in data.columns.levels[0]: data = data.xs('Close', level=0, axis=1)\n",
    "                elif 'Adj Close' in data.columns.levels[0]: data = data.xs('Adj Close', level=0, axis=1)\n",
    "            if isinstance(data, pd.Series): data = data.to_frame()\n",
    "            \n",
    "            # Need High/Low for ADX (Using Target Asset)\n",
    "            ohlc = yf.download(CONFIG['TARGET_ASSET'], start=start, end=end, progress=False)\n",
    "            return data, ohlc\n",
    "        except:\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    def add_features(self, df, ohlc, target=CONFIG['TARGET_ASSET']):\n",
    "        df = df.copy()\n",
    "        \n",
    "        # --- 1. Base Returns ---\n",
    "        df['returns'] = df[target].pct_change()\n",
    "        \n",
    "        # --- 2. Trend Indicators (ADX) ---\n",
    "        df['tr'] = np.maximum(ohlc['High'] - ohlc['Low'], \n",
    "                   np.maximum(abs(ohlc['High'] - ohlc['Close'].shift(1)), \n",
    "                              abs(ohlc['Low'] - ohlc['Close'].shift(1))))\n",
    "        df['dm_plus'] = np.where((ohlc['High'] - ohlc['High'].shift(1)) > (ohlc['Low'].shift(1) - ohlc['Low']), \n",
    "                                 np.maximum(ohlc['High'] - ohlc['High'].shift(1), 0), 0)\n",
    "        df['dm_minus'] = np.where((ohlc['Low'].shift(1) - ohlc['Low']) > (ohlc['High'] - ohlc['High'].shift(1)), \n",
    "                                  np.maximum(ohlc['Low'].shift(1) - ohlc['Low'], 0), 0)\n",
    "        \n",
    "        window = 14\n",
    "        df['tr_s'] = df['tr'].rolling(window).mean()\n",
    "        df['dp_s'] = df['dm_plus'].rolling(window).mean()\n",
    "        df['dm_s'] = df['dm_minus'].rolling(window).mean()\n",
    "        \n",
    "        df['di_plus'] = 100 * (df['dp_s'] / df['tr_s'])\n",
    "        df['di_minus'] = 100 * (df['dm_s'] / df['tr_s'])\n",
    "        df['dx'] = 100 * abs(df['di_plus'] - df['di_minus']) / (df['di_plus'] + df['di_minus'])\n",
    "        df['adx'] = df['dx'].rolling(window).mean()\n",
    "        \n",
    "        # --- 3. Mean Reversion Indicators (Bollinger) ---\n",
    "        sma = df[target].rolling(20).mean()\n",
    "        std = df[target].rolling(20).std()\n",
    "        df['bb_width'] = (std * 2 * 2) / sma\n",
    "        df['pct_b'] = (df[target] - (sma - 2*std)) / (4*std)\n",
    "        df['rsi'] = 100 - (100 / (1 + df['returns'].rolling(14).mean()/df['returns'].rolling(14).std())) # Approx RSI\n",
    "        \n",
    "        # --- 4. Crisis Indicators (VIX, SMA Distance) ---\n",
    "        if '^VIX' in df.columns:\n",
    "            df['vix_norm'] = (df['^VIX'] - 15) / 40\n",
    "        else:\n",
    "            df['vix_norm'] = 0\n",
    "            \n",
    "        sma200 = df[target].rolling(200).mean()\n",
    "        df['dist_sma200'] = (df[target] - sma200) / sma200\n",
    "        \n",
    "        df = df.fillna(0)\n",
    "        return df\n",
    "\n",
    "    def get_data(self, start, end):\n",
    "        df, ohlc = self.download(start, end)\n",
    "        if df.empty: return pd.DataFrame()\n",
    "        return self.add_features(df, ohlc)\n",
    "\n",
    "    def get_crisis_data(self):\n",
    "        # Stitch periods\n",
    "        dfs = []\n",
    "        for s, e in CONFIG['CRISIS_PERIODS']:\n",
    "            d, o = self.download(s, e)\n",
    "            if not d.empty:\n",
    "                dfs.append(self.add_features(d, o))\n",
    "        return pd.concat(dfs).reset_index(drop=True).fillna(0)\n",
    "\n",
    "# ==========================================\n",
    "# 3. UNIFIED AGENT ENVIRONMENT\n",
    "# ==========================================\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, df, mode='trend'):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.mode = mode # 'trend', 'mean_rev', 'crisis'\n",
    "        self.n_features = df.shape[1]\n",
    "        self.window = CONFIG['WINDOW_SIZE']\n",
    "        self.current_step = self.window\n",
    "        \n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.window * self.n_features,), dtype=np.float32)\n",
    "        \n",
    "        self.data = df.values.astype(np.float32)\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.current_step = self.window\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return self.data[self.current_step-self.window : self.current_step].flatten()\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_step >= len(self.df) - 1:\n",
    "            return self._get_obs(), 0, True, False, {}\n",
    "            \n",
    "        act = np.clip(action[0], -1, 1)\n",
    "        \n",
    "        # Get Return (Column 4 is usually 'returns' based on add_features order, \n",
    "        # but let's be safe and assume it's index 0 of features or specific column)\n",
    "        # For simplicity in this demo, we assume 'returns' is the first added feature after base data\n",
    "        # In real code, we'd use column mapping. Here we use the raw price change.\n",
    "        \n",
    "        # Ret Col extraction logic (simple approximation for demo)\n",
    "        ret = self.data[self.current_step, 0] # Returns is usually early in the set\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        # --- REWARD SHAPING PER SPECIALIST ---\n",
    "        if self.mode == 'trend':\n",
    "            # Reward Excess Return\n",
    "            reward = act * ret * 100\n",
    "        elif self.mode == 'mean_rev':\n",
    "            # Reward PnL but punish holding\n",
    "            reward = (act * ret * 100) - (0.1 * abs(act))\n",
    "        elif self.mode == 'crisis':\n",
    "            # Reward Shorting in Drawdowns\n",
    "            reward = (act * ret * 100)\n",
    "            if ret < -0.01 and act < -0.5: reward *= 2.0 # Bonus for catching crash\n",
    "            \n",
    "        self.current_step += 1\n",
    "        return self._get_obs(), reward, False, False, {}\n",
    "\n",
    "# ==========================================\n",
    "# 4. ENSEMBLE MANAGER\n",
    "# ==========================================\n",
    "class EnsembleManager:\n",
    "    def __init__(self):\n",
    "        self.dp = DataProcessor(CONFIG['TICKERS'])\n",
    "        self.agents = {}\n",
    "        self.envs = {}\n",
    "        \n",
    "    def train_specialists(self):\n",
    "        print(\"\\n=== 1. TRAINING SPECIALIST AGENTS ===\")\n",
    "        \n",
    "        # 1. Trend Agent (Bull Market Data)\n",
    "        print(\">> Training Trend Agent (Striker)...\")\n",
    "        trend_data = self.dp.get_data(CONFIG['TRAIN_START'], CONFIG['TRAIN_END'])\n",
    "        env_trend = DummyVecEnv([lambda: TradingEnv(trend_data, mode='trend')])\n",
    "        env_trend = VecNormalize(env_trend, norm_obs=True, norm_reward=False)\n",
    "        model_trend = PPO(\"MlpPolicy\", env_trend, verbose=0, learning_rate=CONFIG['LEARNING_RATE'], device='mps')\n",
    "        model_trend.learn(total_timesteps=CONFIG['TRAINING_STEPS'])\n",
    "        self.agents['trend'] = model_trend\n",
    "        self.envs['trend'] = env_trend # Store for normalization stats\n",
    "        \n",
    "        # 2. Mean Reversion Agent (Chop Data - IWM proxy logic)\n",
    "        print(\">> Training Mean Rev Agent (Midfielder)...\")\n",
    "        # We use same data but different Reward Logic in Env\n",
    "        env_mr = DummyVecEnv([lambda: TradingEnv(trend_data, mode='mean_rev')]) \n",
    "        env_mr = VecNormalize(env_mr, norm_obs=True, norm_reward=False)\n",
    "        model_mr = PPO(\"MlpPolicy\", env_mr, verbose=0, learning_rate=CONFIG['LEARNING_RATE'], device='mps')\n",
    "        model_mr.learn(total_timesteps=CONFIG['TRAINING_STEPS'])\n",
    "        self.agents['mean_rev'] = model_mr\n",
    "        self.envs['mean_rev'] = env_mr\n",
    "        \n",
    "        # 3. Crisis Agent (Crash Data)\n",
    "        print(\">> Training Crisis Agent (Goalkeeper)...\")\n",
    "        crash_data = self.dp.get_crisis_data()\n",
    "        env_crisis = DummyVecEnv([lambda: TradingEnv(crash_data, mode='crisis')])\n",
    "        env_crisis = VecNormalize(env_crisis, norm_obs=True, norm_reward=False)\n",
    "        model_crisis = PPO(\"MlpPolicy\", env_crisis, verbose=0, learning_rate=CONFIG['LEARNING_RATE'], device='mps')\n",
    "        model_crisis.learn(total_timesteps=CONFIG['TRAINING_STEPS'])\n",
    "        self.agents['crisis'] = model_crisis\n",
    "        self.envs['crisis'] = env_crisis\n",
    "        \n",
    "        print(\"=== TRAINING COMPLETE ===\\n\")\n",
    "\n",
    "    def run_backtest(self):\n",
    "        print(\"=== 2. RUNNING ENSEMBLE BACKTEST (2024-2025) ===\")\n",
    "        test_data = self.dp.get_data(CONFIG['TEST_START'], CONFIG['TEST_END'])\n",
    "        if test_data.empty: return\n",
    "        \n",
    "        # Prepare execution variables\n",
    "        balance = CONFIG['INITIAL_BALANCE']\n",
    "        portfolio = balance\n",
    "        holdings = 0\n",
    "        history = []\n",
    "        \n",
    "        # Column mapping for logic\n",
    "        cols = test_data.columns.tolist()\n",
    "        idx_adx = cols.index('adx') if 'adx' in cols else -1\n",
    "        idx_vix = cols.index('vix_norm') if 'vix_norm' in cols else -1\n",
    "        idx_sma = cols.index('dist_sma200') if 'dist_sma200' in cols else -1\n",
    "        idx_ret = cols.index('returns')\n",
    "        \n",
    "        # Convert to numpy for iteration\n",
    "        data_vals = test_data.values\n",
    "        dates = test_data.index\n",
    "        window = CONFIG['WINDOW_SIZE']\n",
    "        \n",
    "        print(f\"{'Date':<12} | {'Regime':<10} | {'Active Agent':<10} | {'VIX':<6} | {'ADX':<6} | {'Action':<6} | {'Balance':<10}\")\n",
    "        print(\"-\" * 90)\n",
    "        \n",
    "        for t in range(window, len(test_data)):\n",
    "            # 1. Observation\n",
    "            obs_raw = data_vals[t-window:t].flatten()\n",
    "            \n",
    "            # 2. Regime Detection\n",
    "            vix = data_vals[t, idx_vix]\n",
    "            adx = data_vals[t, idx_adx]\n",
    "            sma_dist = data_vals[t, idx_sma]\n",
    "            \n",
    "            regime = \"CHOP\"\n",
    "            active_agent_name = \"mean_rev\"\n",
    "            \n",
    "            # --- GATING LOGIC ---\n",
    "            if vix > 0.5 or sma_dist < -0.05: # High VIX or Price 5% below SMA200\n",
    "                regime = \"CRISIS\"\n",
    "                active_agent_name = \"crisis\"\n",
    "            elif adx > 25:\n",
    "                regime = \"TREND\"\n",
    "                active_agent_name = \"trend\"\n",
    "            else:\n",
    "                regime = \"CHOP\"\n",
    "                active_agent_name = \"mean_rev\"\n",
    "                \n",
    "            # 3. Agent Selection & Prediction\n",
    "            agent = self.agents[active_agent_name]\n",
    "            norm_env = self.envs[active_agent_name]\n",
    "            \n",
    "            # Normalize observation using the specific agent's training stats\n",
    "            obs_norm = norm_env.normalize_obs(obs_raw)\n",
    "            action, _ = agent.predict(obs_norm, deterministic=True)\n",
    "            \n",
    "            # 4. Execution\n",
    "            mkt_ret = data_vals[t, idx_ret]\n",
    "            position_size = np.clip(action[0], -1, 1)\n",
    "            \n",
    "            # Fee logic\n",
    "            turnover = abs(position_size - holdings)\n",
    "            cost = turnover * CONFIG['FEES']\n",
    "            \n",
    "            pnl_pct = (position_size * mkt_ret) - cost\n",
    "            portfolio *= (1 + pnl_pct)\n",
    "            holdings = position_size\n",
    "            \n",
    "            # 5. Logging\n",
    "            if t % 10 == 0: # Print every 10 days to keep output clean but visible\n",
    "                print(f\"{str(dates[t].date()):<12} | {regime:<10} | {active_agent_name:<10} | {vix:<6.2f} | {adx:<6.2f} | {position_size:<6.2f} | {portfolio:<10.0f}\")\n",
    "                \n",
    "            history.append({\n",
    "                'Date': dates[t],\n",
    "                'Portfolio': portfolio,\n",
    "                'Regime': regime,\n",
    "                'Agent': active_agent_name,\n",
    "                'Return': pnl_pct,\n",
    "                'Benchmark': mkt_ret\n",
    "            })\n",
    "            \n",
    "        # --- Final Stats ---\n",
    "        res = pd.DataFrame(history).set_index('Date')\n",
    "        res['Bench_Equity'] = (1 + res['Benchmark']).cumprod() * CONFIG['INITIAL_BALANCE']\n",
    "        \n",
    "        total_ret = (portfolio / CONFIG['INITIAL_BALANCE']) - 1\n",
    "        bench_ret = (res['Bench_Equity'].iloc[-1] / CONFIG['INITIAL_BALANCE']) - 1\n",
    "        \n",
    "        print(\"\\n=== FINAL ENSEMBLE REPORT ===\")\n",
    "        print(f\"Total Return: {total_ret:.2%} (Benchmark: {bench_ret:.2%})\")\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(res['Portfolio'], label='Ensemble AI', linewidth=2)\n",
    "        plt.plot(res['Bench_Equity'], label='SPY Buy & Hold', linestyle='--', color='gray')\n",
    "        \n",
    "        # Color background by Regime\n",
    "        # (Simplified visualization for Regime)\n",
    "        # We can't easily paint background per day in simple matplotlib without complex code,\n",
    "        # but the Console Output serves the debug purpose well.\n",
    "        \n",
    "        plt.title(\"Ensemble System (Trend + MeanRev + Crisis) vs SPY\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mgr = EnsembleManager()\n",
    "    mgr.train_specialists()\n",
    "    mgr.run_backtest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
